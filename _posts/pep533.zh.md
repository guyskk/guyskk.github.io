# PEP 533 -- Deterministic cleanup for iterators

```
PEP:	533
Title:	Deterministic cleanup for iterators
Author:	Nathaniel J. Smith
Status:	Draft
Type:	Standards Track
Created:	18-Oct-2016
Post-History:	18-Oct-2016
```


## Abstract

We propose to extend the iterator protocol with a new __(a)iterclose__ slot, which is called automatically on exit from (async) for loops, regardless of how they exit. This allows for convenient, deterministic cleanup of resources held by iterators without reliance on the garbage collector. This is especially valuable for asynchronous generators.

我们提议拓展迭代器协议，添加一个新的 `__(a)iterclose__` 特殊方法，
这个方法会在 for 循环退出时自动调用，无论循环是如何退出的。
这样就能方便的，稳妥的（确定性的）关闭由迭代器持有的资源而不需要依赖于垃圾回收器。
这个提议对异步生成器而言尤其重要。

## Note on timing

In practical terms, the proposal here is divided into two separate parts: the handling of async iterators, which should ideally be implemented ASAP, and the handling of regular iterators, which is a larger but more relaxed project that can't start until 3.7 at the earliest. But since the changes are closely related, and we probably don't want to end up with async iterators and regular iterators diverging in the long run, it seems useful to look at them together.

在实践中，这个提议分为两部分：异步迭代器，需要尽快实现，
以及普通生成器，这是一个更大但不那么紧急的项目，最早只能在 3.7 之后开始。
但是因为这两部分密切相关，我们不希望异步迭代器和普通迭代器最后出现分歧，把它们一起看待会比较好。


## Background and motivation

Python iterables often hold resources which require cleanup. For example: file objects need to be closed; the WSGI spec adds a close method on top of the regular iterator protocol and demands that consumers call it at the appropriate time (though forgetting to do so is a frequent source of bugs); and PEP 342 (based on PEP 325) extended generator objects to add a close method to allow generators to clean up after themselves.

Python 可迭代对象通常会持有需要关闭的资源。例如：文件对象需要关闭；
WSGI 标准在普通迭代器之上添加了 close 方法，并且要求使用者在合适的时机调用它
（忘记这么做是导致 Bug 的一个常见原因）。
并且 PEP 342 (基于 PEP 325) 拓展了生成器对象，添加了 close 方法以允许自行清理。

Generally, objects that need to clean up after themselves also define a __del__ method to ensure that this cleanup will happen eventually, when the object is garbage collected. However, relying on the garbage collector for cleanup like this causes serious problems in several cases:

通常，需要自行清理的 Python 对象也会定义一个 `__del__` 方法以确保清理最终会在垃圾回收时执行。
然而，依赖垃圾回收来执行清理在一些情况下会导致严重的问题：

- In Python implementations that do not use reference counting (e.g. PyPy, Jython）, calls to __del__ may be arbitrarily delayed -- yet many situations require prompt cleanup of resources. Delayed cleanup produces problems like crashes due to file descriptor exhaustion, or WSGI timing middleware that collects bogus times.

- 在不使用引用计数的 Python 实现（例如：PyPy, Jython）中，`__del__` 调用可能会任意延后 --
很多情况下需要提示 GC 才会执行清理。延后清理会造成文件描述符耗尽导致程序崩溃，
或 WSGI 计时中间件收集到错误的计时。

- Async generators (PEP 525) can only perform cleanup under the supervision of the appropriate coroutine runner. __del__ doesn't have access to the coroutine runner; indeed, the coroutine runner might be garbage collected before the generator object. So relying on the garbage collector is effectively impossible without some kind of language extension. (PEP 525 does provide such an extension, but it has a number of limitations that this proposal fixes; see the "alternatives" section below for discussion.)

- 异步生成器 (PEP 525) 只能在适当的协程执行器中执行清理，但 `__del__` 无法获取协程执行器。
实际上，协程执行器可能比生成器更早就被垃圾回收了。
所以，不拓展语言的话不可能依赖垃圾回收器执行清理。（PEP 525 确实提供了这样一个拓展，
但它有许多局限，具体见 "alternatives" 小节的讨论。）

Fortunately, Python provides a standard tool for doing resource cleanup in a more structured way: with blocks. For example, this code opens a file but relies on the garbage collector to close it:

幸运的是，Python 提供了一个标准的工具来执行资源清理：with 语句。
例如，以下代码打开一个文件但依赖垃圾回收关闭它：

```python
def read_newline_separated_json(path):
    for line in open(path):
        yield json.loads(line)

for document in read_newline_separated_json(path):
    ...

```

and recent versions of CPython will point this out by issuing a ResourceWarning, nudging us to fix it by adding a with block:

并且当前版本的 CPython 会通过产生 ResourceWarning 指出这个问题，
促使我们用 with 语句修复它：

```python
def read_newline_separated_json(path):
    with open(path) as file_handle:      # <-- with block
        for line in file_handle:
            yield json.loads(line)

for document in read_newline_separated_json(path):  # <-- outer for loop
    ...
```

But there's a subtlety here, caused by the interaction of with blocks and generators. with blocks are Python's main tool for managing cleanup, and they're a powerful one, because they pin the lifetime of a resource to the lifetime of a stack frame. But this assumes that someone will take care of cleaning up the stack frame... and for generators, this requires that someone close them.

但是同时使用 with 语句和生成器会导致一个微妙的问题。
with 语句是 Python 处理清理步骤的主要工具，也是一个强大的工具，
因为它把资源的生命周期固定在一个栈帧。但是这假设有某个东西会清理这个栈帧... 对于生成器，
这需要有个关闭它。

In this case, adding the with block is enough to shut up the ResourceWarning, but this is misleading -- the file object cleanup here is still dependent on the garbage collector. The with block will only be unwound when the read_newline_separated_json generator is closed. If the outer for loop runs to completion then the cleanup will happen immediately; but if this loop is terminated early by a break or an exception, then the with block won't fire until the generator object is garbage collected.

这种情况下，添加 with 语句可以消除 ResourceWarning，但这是个误导 --
这个文件对象的清理依然依赖于垃圾回收器。这个 with 块只会在 `read_newline_separated_json`
生成器关闭时解除。如果外部的 for 循环结束清理步骤会立即执行，但是如果循环由于 break
语句或异常提前结束，with 块就不会执行，直到生成器被垃圾回收。

The correct solution requires that all users of this API wrap every for loop in its own with block:

正确的方式需要所有API使用者把所有循环包裹在自己的 with 块中：

```python
with closing(read_newline_separated_json(path)) as genobj:
    for document in genobj:
        ...
```

This gets even worse if we consider the idiom of decomposing a complex pipeline into multiple nested generators:

如果考虑多层嵌套的生成器，情况会更糟：

```python
def read_users(path):
    with closing(read_newline_separated_json(path)) as gen:
        for document in gen:
            yield User.from_json(document)

def users_in_group(path, group):
    with closing(read_users(path)) as gen:
        for user in gen:
            if user.group == group:
                yield user
```

In general if you have N nested generators then you need N+1 with blocks to clean up 1 file. And good defensive programming would suggest that any time we use a generator, we should assume the possibility that there could be at least one with block somewhere in its (potentially transitive) call stack, either now or in the future, and thus always wrap it in a with. But in practice, basically nobody does this, because programmers would rather write buggy code than tiresome repetitive code. In simple cases like this there are some workarounds that good Python developers know (e.g. in this simple case it would be idiomatic to pass in a file handle instead of a path and move the resource management to the top level), but in general we cannot avoid the use of with/finally inside of generators, and thus dealing with this problem one way or another. When beauty and correctness fight then beauty tends to win, so it's important to make correct code beautiful.

通常，如果你有 N 个嵌套的生成器，那么你要 N+1 个 with 块来清理一个文件。
好的防御性编程建议在任何时候使用生成器，我们可以在每个调用栈加上一个 with 块，
这样确保生成器总是在包裹 with 块中。但实践中，几乎没人这么做，因为程序员宁可写有 bug
的代码也不写烦人的重复代码。在这种简单的情况下有一些避开问题的方式（例如：
传递一个文件对象而不是文件路径，把资源管理移到顶层函数），
但我们无法避免在生成器中使用 with/finally，因此无论如何总要解决这个问题。
当美和正确冲突，美倾向于胜出，所以使正确的代码更美非常重要。

Still, is this worth fixing? Until async generators came along I would have argued yes, but that it was a low priority, since everyone seems to be muddling along okay -- but async generators make it much more urgent. Async generators cannot do cleanup at all without some mechanism for deterministic cleanup that people will actually use, and async generators are particularly likely to hold resources like file descriptors. (After all, if they weren't doing I/O, they'd be generators, not async generators.) So we have to do something, and it might as well be a comprehensive fix to the underlying problem. And it's much easier to fix this now when async generators are first rolling out, then it will be to fix it later.

最后，这个问题值得去修复吗？在异步生成器出现之前我认为值得，但优先级很低，
因为所有人看起来都对它含糊不清 -- 但异步生成器使它更紧迫。没有一个确定的清理机制，
异步生成器根本无法自行清理，并且异步生成器常常持有资源对象，例如文件描述符。（毕竟，
如果不需要处理 I/O，它们可以写成普通生成器，而不是异步生成器。）
所以我们不得不做点什么，并且它可能需要一个综合的修复。现在这个问题更容易修复了，
因为异步生成器已经推出。

The proposal itself is simple in concept: add a __(a)iterclose__ method to the iterator protocol, and have (async) for loops call it when the loop is exited, even if this occurs via break or exception unwinding. Effectively, we're taking the current cumbersome idiom (with block + for loop) and merging them together into a fancier for. This may seem non-orthogonal, but makes sense when you consider that the existence of generators means that with blocks actually depend on iterator cleanup to work reliably, plus experience showing that iterator cleanup is often a desireable feature in its own right.

这个提议本身很简单：给迭代器协议添加一个 `__(a)iterclose__` 方法，并使 (async) for
循环退出时调用它，即使循环通过 break 或异常退出。
简而言之，我们取出麻烦的 (with block + for loop) 并合并成一个更漂亮的 for 循环。
这看似耦合了，但当你考虑到生成器的存在意味着 with 块实际上取决于迭代器可靠地关闭，
这就有意义了，再加上实际经验显示迭代器自行清理常常是一个有利的特性。

## Alternatives

### PEP 525 asyncgen hooks

PEP 525 proposes a set of global thread-local hooks managed by new ``sys.{get/set}_asyncgen_hooks()` functions <https://www.python.org/dev/peps/pep-0525/#finalization>`_, which allow event loops to integrate with the garbage collector to run cleanup for async generators. In principle, this proposal and PEP 525 are complementary, in the same way that with blocks and __del__ are complementary: this proposal takes care of ensuring deterministic cleanup in most cases, while PEP 525's GC hooks clean up anything that gets missed. But __aiterclose__ provides a number of advantages over GC hooks alone:

PEP 525 