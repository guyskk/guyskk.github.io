
# 关于AI产品，分享一些观点

## 关于大模型

大模型能力：
参数量：1B ~ 1000B
GPT3/GPT3.5 175B
智能程度：
小模型 qwen 0.6B 比GPT3 还更好，指的是通用能力，根据我实际和AI进行对话的使用体验和感受。

大模型会越来越聪明。
各大厂商疯狂卷，一边说到瓶颈了，一边取得重大突破。
相信世界上这么多聪明人，会让大模型越来越聪明，直到全面超越人类。
人脑约860亿个神经元，约100万亿个突触连接（现在大模型的100倍）。

大模型会越来越便宜。
小模型能力也越来越强，MOE本质上是激活参数=小模型的大模型，激活的参数少，省钱，速度更快。
做AI应用只关心推理成本和智能水平，不关心训练成本。大部分MOE的激活参数在7B或32B，智能程度很好。
市场竞争非常激烈，价格会无限接近成本。
开源模型会接近甚至超越闭源模型，相信国产开源模型。
头部顶尖的闭源模型比较难超越，但目前 deepseek, kimi-k2 的能力已经比较接近了。非头部的闭源模型早已被最好的开源模型超越。

大模型变强之后，提示词优化没有那么重要了
更重要的是上下文工程，给AI提供充分且必要的上下文
把任务和需求说清楚，把操作流程说清楚，AI就能很好的遵循
提示词里使用XML标签区分不同的部分，避免歧义，这个很重要，也很简单
强的大模型是一点就通，能理解很复杂的需求，重点是上下文是否充足。
弱一点的模型，脑子转不过来，注意力有限，提示词不能太复杂，任务要简单一点，一次只做一件事。

智能的要素 - Loop/循环
大模型经常犯错误，人类同样经常犯错误，不要期望一步就能把事情做对。
对于Workflow模式，如果每一步的正确率90%，那么5步之后准确率就只有60%了，错误会指数级积累。
每一步90%的正确率，串行5次后，错误累积，最后正确率60%。而并行5次呢，错误稀释，正确率大于90%。
构建Agent就是要构建反馈循环，让AI能够验证每一步，出错了可以修正，不断的尝试和改进，直到完成任务。
人类的智能本质上也是一样的，发现错了可以修复、调整思路、或者重新来，本质的差异是人类拥有完整的上下文和环境感知，可以轻松的验证步骤和结果，而AI Agent常常无法验证步骤和结果。

## 关于AI应用

AI应用就是套壳，把套壳做到极致就是牛逼。by 肖弘
以前套壳是做对话聊天，写提示词，套openai API的壳。
套壳做工作流，coze, dify, n8n 都是。
现在套壳做Agent，manus, coze空间, claude code, codex。

关于"套壳"的重新定义，"套壳"说得有点贬义了。好的套壳其实是产品化能力：
把复杂的AI能力包装成普通用户能用的产品
解决最后一公里的用户体验问题
很多时候，这比底层技术更难，更有价值

大模型像水位一样，不断上涨。AI应用要像一艘船，大模型越强则AI应用也越强，水涨船高，不要做固定在水底的柱子（例如绑定在某种技术上，难以随着大模型进化而一起变强的应用）。
核心在于架构设计时就要考虑模型能力的可替换性和可升级性

大厂都在卷AI应用，尤其是AI编程，这是非常值钱的市场，因为程序员很贵。
AI能力到一定程度（目前在临界点，AI能独立的完成复杂的编程任务了），编程范式就改变了
以前是主要人来写代码，AI辅助生成，需要比较频繁的人机协作和AI沟通，AI只能完成很简单任务。
现在是人主要做需求和方案沟通，像技术管理者一样，给AI分配任务，人变得很少亲自写代码，AI能完成更长的更复杂的任务，更能独立的完成任务，不需要经常人来纠正和沟通。

软件的开发成本会越来越低，软件会越来越不值钱，大家都能很快开发出各种软件。
知识作为一种生产要素，会被AI极大的丰富，供给无限。

开源的软件太多了，大厂都下场开源，更新迭代频率极高，只有套壳才能跟上节奏，自己研发太慢了。
像Manus这样牛逼的产品，还没正式发布就出了很多开源替代，时间窗口非常短。
cursor有一堆开源替代，大厂全部做了竞品。
claude code哪怕他已经是开源的，也有一堆开源替代，其他大厂同样都做了竞品。

技术本身的壁垒很小，模型开源的，应用也是开源的。就看谁更卷。


## 关于AI产品

对于传统需求的AI改造，AI产品=90%老产品+10%的AI能力，用户不会为AI额外付费，但是同行有AI你不能没有。
确实有原生的AI产品，但是toB很多业务需求还是传统需求，原生AI很少见。

关于中间商的价值重估
赚钱的： 卖课，李一舟， 倒卖会员， 倒卖API
AI可能让新型中间商变得更有价值：
AI能力的集成商和经销商
跨模型的统一接口和管理平台
AI应用的分发和运营平台

有什么真正的护城河吗？
数据，独一无二的数据，持续更新的数据
用户数据，用户习惯，例如ChatGPT，主要适用于C端
脏活累活才是护城河。
脏活累活指的是长尾需求，需要行业know-how的垂直领域的需求，包括数据爬虫、清洗整理、客户的复杂数据和业务逻辑、行业规则等。并不是低毛利的，堆人工的无法扩展的业务。
"数据爬虫、清洗整理、客户的复杂数据和业务逻辑"听起来就是苦力活。如果这真能成为护城河，核心在于：把一次性苦力活转化为可复用的资产，比如爬下来的数据能成为行业数据，清洗逻辑能产品化为专属知识库。

公开数据的价值，就像数字石油一样
持续更新的爬虫，清洗、结构化之后的数据，大家都想要
例如天眼查/企查查的企业数据，例如5188的关键词大数据，社媒榜单数据
更深层的数据价值：
时序数据 - 不只是静态快照，而是变化趋势和模式
关联数据 - 单点数据容易被复制，但多维度关联很难
真实验证数据 - AI生成内容泛滥后，真实性验证的数据会更值钱

关于网络效应的可能性，AI时代的网络效应可能有新形式：
用户生成的prompt和工作流可以形成社区效应
多Agent协作可能产生新的网络价值
例如游戏/社交相关的AI Agent产品

大厂都会做大模型，大厂也都会做AI应用
通用领域的需求，会被大厂的炮火覆盖。例如：对话问答、编程、Manus类产品。
理想情况下，大模型越来越强，AI应用也越做越强，通用型AI将会满足绝大部分需求。
剩下的解决不好的问题，也就是创业者的机会，是什么？
就剩下脏活累活了。

创新者的窘境
大厂也有船大难掉头的问题，官僚主义
大厂有内部政治和KPI导向，可能错过真正的用户需求
大厂很难做真正的dirty work，因为投入产出比不够性感
当大家都知道方向的时候，大厂会以10倍百倍的资源投入，全方位炮火覆盖，确保占领市场。
当大家都没有方向的时候，你敢押注一个非共识的方向吗？

Kimi（月之暗面）本来想做ChatGPT，干不过字节跳动和腾讯。
Kimi现在做Claude平替，大厂也很快跟进了，时间窗口很短。
当非共识变成共识，你的领先优势很快就会消失。

怎样才能赢呢？
可能唯一的出路是拥抱竞争，卷死同行。
不要和大厂交锋，肯定卷不过。
找软柿子捏，和小绵羊竞争，找到一个足够大（有规模），又足够小（不会被大厂干掉）的市场（垂直细分市场，并且不会被通用型产品覆盖），把竞争者都干掉，成为这个垂直领域的垄断者。
要找"足够大（有规模），又足够小（不会被大厂干掉）"的市场，我也不知道在哪，哪里去找？


关于垂直细分市场的边界，我觉得关键可能是几个维度：
数据孤岛 - 某些行业的数据不互通，需要深入一线获取，比如制造业的设备数据
决策链复杂 - toB决策周期长，需要长期客户关系维护，大厂不一定有耐心
定制化程度高 - 标准化产品解决不了，需要大量定制开发
但现实中很多垂直领域大厂也在布局，比如阿里的工业互联网、腾讯的医疗AI。这种情况下，创业者的时间窗口是不是更短？

竞争的过程，就是形成护城河的过程。
大家比拼的是什么？要形成能持续增长的护城河，需要积累什么？
数据飞轮（越多客户→越多数据→模型越好）

关于价值创造的本质
还有一个角度：不是所有价值都来自技术领先
有些价值来自整合 - 把分散的工具和服务整合起来
有些价值来自降维打击 - 用AI把原来复杂的事情变简单
有些价值来自新场景创造 - 之前不存在的需求
